{"cells":[{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":163658,"status":"ok","timestamp":1694949035452,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"ojnGvoh-MqGw"},"outputs":[],"source":["from torchvision.transforms import transforms\n","from torch.utils.data import Dataset\n","import pickle\n","import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import natsort\n","#set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#constants\n","VIDEO = 40\n","PARTICIPANT = 32\n","CHANNEL = 32\n","SESSION = 63\n","STAT_FEATURE_MAX_LEN = 58\n","IMAGE_PIXEL_NUM = 496*369\n","BAND_TYPE_NUM = 4\n","\n","\n","#Make Datasets\n","class Arousal_Dataset(Dataset):\n","    def __init__(self, image_dir_path, train = None):\n","        super().__init__\n","        self.image_dir_path = image_dir_path\n","        image_name_list = os.listdir(image_dir_path)\n","        self.image_name_list = natsort.natsorted(image_name_list)\n","\n","    def __getitem__(self, index):\n","        ##image data\n","        #get image of theta to gamma\n","        #dictionary \"band type\" ->  tensor image\n","        band_images = {}\n","        band_types = ['alpha', 'beta', 'gamma', 'theta']\n","        for i in range(4):\n","            image_path = os.path.join(self.image_dir_path, self.image_name_list[4*index + i])\n","            image = np.array(Image.open(image_path))\n","            #remove alpha channel from .png file\n","            image = image[:,:,:3]\n","            tensor_image = torch.from_numpy(image).permute(2,0,1).float()\n","            band_images[band_types[i]] = tensor_image\n","\n","        image_name_split_list = self.image_name_list[4*index].split('_')\n","\n","        #locate information of files\n","        par, vid, cha, ses = int(image_name_split_list[0]), int(\n","            image_name_split_list[1]), int(image_name_split_list[2]), int(image_name_split_list[3])\n","        \n","        # statistic data,[Par, Vid, Cha, Ses] (32,40,32,15)\n","        with open('./Data/Statistic_Features_np.pkl', 'rb') as f:\n","            statistic_features = pickle.load(f)\n","\n","        statistic_feature = statistic_features[par-1][vid-1][cha-1][ses-1]\n","        #elong the legnth to (,58)\n","        statistic_feature = np.pad(statistic_feature, (0, STAT_FEATURE_MAX_LEN - len(statistic_feature)), 'constant', constant_values= 0)\n","        tensor_statistic_feature = torch.from_numpy(statistic_feature).float()\n","        \n","        #band images : dictionary, tensor images\n","        #tensor statistic feature : tensor 1 dim array\n","        x_data = band_images, tensor_statistic_feature\n","\n","        ##truth ground data form file name\n","        Truth_label = image_name_split_list[5]\n","\n","        if Truth_label[0 :2] == 'HA':\n","            y_data = 1\n","        else:\n","            y_data = 0\n","\n","        return x_data, y_data\n","\n","    def __len__(self):\n","        return int(len(self.image_name_list)/4)\n","\n","# #데이터 전처리\n","# transform = transforms.Compose(\n","#     [transforms.ToTensor(),transforms.Normalize((0.5 , 0.5, 0.5), (0.5, 0.5, 0.5))]\n","# )\n","\n","\n","#총 데이터 num :32 * 40 * 32 * 15 * 4(band), batch_size는 이의 약수, 논문에 명시됨\n","batch_size = 16\n","Data_Path_Train = \"./Data/Train_Data/\"\n","Data_Path_Validation = \"./Data/Validation_Data/\"\n","Data_Path_Test = \"./Data/Test_Data/\"\n","\n","\n","\n","#test_데이터, train_데이터 불러오고 저장\n","train_set = Arousal_Dataset(image_dir_path = Data_Path_Train, train = True)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers = 0)\n","\n","test_set = Arousal_Dataset(image_dir_path = Data_Path_Test, train = False)\n","\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n","\n","validation_set = Arousal_Dataset(image_dir_path= Data_Path_Validation, train = False)\n","\n","validation_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n","#output clasees\n","classes =  (0, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":446,"status":"ok","timestamp":1694949074035,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"AKK44tOYMqGx"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MSCB_CNN(nn.Module):\n","    def __init__(self):   \n","        super(MSCB_CNN, self).__init__()\n","\n","        ###for MSCB\n","        self.conv_L = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,5) , stride = 1, padding = 'same')\n","        self.conv_M = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,3), stride = 1, padding = 'same')\n","        self.conv_S = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        self.conv_raw = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        \n","        #Stride = 5 -> 그렇기에 kernel size 또한 5, 그러나 데이터의 규격이 5의 배수가 아니여서 4로 조정, same padding : (kernerl_size/2, kerenel_size/2-1)\n","        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4, padding = (2,1))\n","\n","        # Convolutional layers\n","        self.conv = nn.Conv2d(in_channels = 56, out_channels = 112, kernel_size=(1,3), stride = 1, padding=3)\n","\n","        #flatten layer\n","        self.flat = nn.Flatten()\n","\n","        # Fully connected layers, 112 : final kernel num, /(4*4) striding number at pooling\n","        self.fc1 = nn.Linear(358458, 400)\n","        self.fc2 = nn.Linear(400, 300)  \n","        # 2 output classes (0 and 1)«\n","        self.fc3 = nn.Linear(300, 2)\n","\n","        # Dropout layer to prevent overfitting\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","        \n","    def forward(self, x):\n","        #MSCB Block\n","        def MSCB(self, x):\n","            x_L = self.pool((torch.relu(self.conv_L(x))))\n","            x_M = self.pool((torch.relu(self.conv_M(x))))\n","            x_S = self.pool((torch.relu(self.conv_S(x))))\n","            x_raw = self.conv_raw((torch.relu(self.pool(x))))\n","\n","            ##2차원 이미지 합치기, kernerl수 증가\n","            x = torch.cat((x_L, x_M, x_S, x_raw), dim = 1)\n","\n","            return x\n","        \n","        def Conv_to_FCL(self,x):\n","            x = self.pool(torch.relu(self.conv(x)))\n","            x = self.flat(x)\n","\n","            return x\n","            \n","        # print(f\"initial x {x.shape}\")\n","        band_images, tensor_statistic_feature = x\n","        band_images['theta'], band_images['alpha'], band_images['beta'], band_images['gamma'], tensor_statistic_feature = band_images['theta'].to(device), band_images['alpha'].to(device), band_images['beta'].to(device), band_images['gamma'].to(device), tensor_statistic_feature.to(device) \n","        theta_MSCB = MSCB(self, band_images[\"theta\"])\n","        alpha_MSCB = MSCB(self, band_images[\"alpha\"])\n","        beta_MSCB = MSCB(self, band_images[\"beta\"])\n","        gamma_MSCB = MSCB(self, band_images[\"gamma\"])\n","        # print(f\"MSCB shape : {theta_MSCB.shape}\")\n","\n","        theta_features = Conv_to_FCL(self, theta_MSCB)\n","        alpha_features = Conv_to_FCL(self, alpha_MSCB)\n","        beta_features = Conv_to_FCL(self, beta_MSCB)\n","        gamma_features = Conv_to_FCL(self, gamma_MSCB)\n","        \n","        all_feature_map = torch.cat((theta_features,alpha_features,beta_features,gamma_features,tensor_statistic_feature), dim = 1)\n","\n","        x = torch.relu(self.fc1(all_feature_map))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x\n","\n","#initialize with MPS GPU\n","net = MSCB_CNN().to(device)\n","\n","\n","#Set Optimizer and loss function\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay = 0.02)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.5)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6c1a9e6e948346e08db35c1cdaa98142","57db90486c05493696ccd5dd12e1fa76","83f6d083d59f4370adbf5ed05163fe80","f3fc3fa138614c63849cc9788bcdfb63","5ae44592655b441b8a25dae83e08f514","4106fcd31a1b4ab8b7ea3a73d4feef59","8d439083db004ee2ad1e390d5498967c","1a37b57b03c141afb23bd96c9c60f2c8","44f30e031c6a4eb4ba80c4c47c11fcb2","6402a280118a477eb43772732850421e","cbbe9289e69b4786820ddb7f88d860ce"]},"id":"wK6lrJiHMqGx","outputId":"7c020a1b-5177-48b6-efbf-e6ca688f720f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd0a416084864f95b5f19b856fd3979a","version_major":2,"version_minor":0},"text/plain":["epoch:   0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bc07128fe61434fac3ed83a14b5ac07","version_major":2,"version_minor":0},"text/plain":["learning!!!:   0%|          | 0/15311 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["start learning!!!\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n","torch.Size([16, 14, 93, 124])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\USER\\Desktop\\python_codes\\MSCB_CNN-main\\MSCB_CNN_Aro.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epochs), desc\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m):   \u001b[39m# 데이터셋을 수차례 반복합니다.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader, desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlearning!!!\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mif\u001b[39;00m start_flag \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstart learning!!!\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\palette\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","\u001b[1;32mc:\\Users\\USER\\Desktop\\python_codes\\MSCB_CNN-main\\MSCB_CNN_Aro.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# statistic data,[Par, Vid, Cha, Ses] (32,40,32,15)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./Data/Statistic_Features_np.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     statistic_features \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m statistic_feature \u001b[39m=\u001b[39m statistic_features[par\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][vid\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][cha\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][ses\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/python_codes/MSCB_CNN-main/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m#elong the legnth to (,58)\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm.notebook import tqdm\n","#학습\n","num_epochs = 500\n","start_flag = 1\n","for epoch in tqdm(range(num_epochs), desc= \"epoch\"):   # 데이터셋을 수차례 반복합니다.\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(tqdm(train_loader, desc = \"learning!!!\"), 0):\n","\n","        if start_flag == 1:\n","            print(\"start learning!!!\")\n","            start_flag = 0\n","\n","        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n","        inputs, labels = data\n","\n","        #검증\n","        # print(f\"minibatch number : {i}, file's shape : {inputs.shape}\")\n","\n","        #move to GPU to calculate, inputs는 tuple, dict로 포장되어있기에 모델 내에서 .to(device)\n","        labels = labels.to(device)\n","        \n","        # 변화도(Gradient) 매개변수를 0으로 만들고\n","        optimizer.zero_grad()\n","\n","        # 순전파 + 역전파 + 최적화를 한 후\n","        outputs = net(inputs)\n","        # print(outputs.shape)\n","        # print(labels)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 통계를 출력합니다.\n","        running_loss += loss.item()\n","        if i % 600 == 599:    # print every 600 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","            \n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m55gfISW24vm"},"outputs":[],"source":["# ##Save Model\n","# PATH = \"/content/drive/MyDrive/models/\"\n","# torch.save(net.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEFECQ1bMqGy"},"outputs":[],"source":["#Accuracy Check\n","correct = 0\n","total = 0\n","# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n","with torch.no_grad():\n","    for data in validation_loader:\n","        images, labels = data\n","        labels =labels.to(device)\n","        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n","        outputs = net(images).to(device)\n","        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct / total : .2f} %')"]},{"cell_type":"markdown","metadata":{},"source":["***결과 정리 노트***\n","None : 58.74\n","Laplace : 41.09\n","CAR : 56.13"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"1a37b57b03c141afb23bd96c9c60f2c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4106fcd31a1b4ab8b7ea3a73d4feef59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f30e031c6a4eb4ba80c4c47c11fcb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57db90486c05493696ccd5dd12e1fa76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4106fcd31a1b4ab8b7ea3a73d4feef59","placeholder":"​","style":"IPY_MODEL_8d439083db004ee2ad1e390d5498967c","value":" 13%"}},"5ae44592655b441b8a25dae83e08f514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6402a280118a477eb43772732850421e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1a9e6e948346e08db35c1cdaa98142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57db90486c05493696ccd5dd12e1fa76","IPY_MODEL_83f6d083d59f4370adbf5ed05163fe80","IPY_MODEL_f3fc3fa138614c63849cc9788bcdfb63"],"layout":"IPY_MODEL_5ae44592655b441b8a25dae83e08f514"}},"83f6d083d59f4370adbf5ed05163fe80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a37b57b03c141afb23bd96c9c60f2c8","max":1394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44f30e031c6a4eb4ba80c4c47c11fcb2","value":184}},"8d439083db004ee2ad1e390d5498967c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbbe9289e69b4786820ddb7f88d860ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3fc3fa138614c63849cc9788bcdfb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6402a280118a477eb43772732850421e","placeholder":"​","style":"IPY_MODEL_cbbe9289e69b4786820ddb7f88d860ce","value":" 184/1394 [1:08:51&lt;7:32:00, 22.41s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
