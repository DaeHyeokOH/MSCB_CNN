{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":163658,"status":"ok","timestamp":1694949035452,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"ojnGvoh-MqGw"},"outputs":[],"source":["from torchvision.transforms import transforms\n","from torch.utils.data import Dataset\n","import pickle\n","import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import natsort\n","#set device\n","device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n","#constants\n","VIDEO = 40\n","PARTICIPANT = 32\n","CHANNEL = 32\n","SESSION = 63\n","STAT_FEATURE_MAX_LEN = 58\n","IMAGE_PIXEL_NUM = 496*369\n","BAND_TYPE_NUM = 4\n","\n","\n","#Make Datasets\n","class Arousal_Dataset(Dataset):\n","    def __init__(self, image_dir_path, stat_data_path, train = None):\n","        super().__init__\n","        self.image_dir_path = image_dir_path\n","        self.stat_data_path  = stat_data_path\n","        image_name_list = os.listdir(image_dir_path)\n","        self.image_name_list = natsort.natsorted(image_name_list)\n","\n","    def __getitem__(self, index):\n","        #Tensor Transformer\n","        to_tensor = transforms.ToTensor()\n","        ##image data\n","        #get image of theta to gamma\n","        #dictionary \"band type\" ->  tensor image\n","        band_images = {}\n","        band_types = ['alpha', 'beta', 'gamma', 'theta']\n","        for i in range(4):\n","            image_path = os.path.join(self.image_dir_path, self.image_name_list[4*index + i])\n","            image = np.array(Image.open(image_path))\n","            tensor_image = to_tensor(image)\n","            band_images[band_types[i]] = tensor_image\n","\n","        image_name_split_list = self.image_name_list[4*index].split('_')\n","\n","        #locate information of files\n","        par, vid, cha, ses = int(image_name_split_list[0]), int(\n","            image_name_split_list[1]), int(image_name_split_list[2]), int(image_name_split_list[3])\n","        \n","        # statistic data,[Par, Vid, Cha, Ses] (32,40,32,15)\n","        with open('./Data/Statistic_Features_np.pkl', 'rb') as f:\n","            statistic_features = pickle.load(f)\n","\n","        statistic_feature = statistic_features[par-1][vid-1][cha-1][ses-1]\n","        #elong the legnth to (,58)\n","        statistic_feature = np.pad(statistic_feature, (0, STAT_FEATURE_MAX_LEN - len(statistic_feature)), 'constant', constant_values= 0)\n","        tensor_statistic_feature = to_tensor(statistic_feature)\n","        \n","        #band images : dictionary, tensor images\n","        #tensor statistic feature : tensor 1 dim array\n","        x_data = band_images, tensor_statistic_feature\n","\n","        ##truth ground data form file name\n","        Truth_label = image_name_split_list[5]\n","\n","        if Truth_label[0 :2] == 'HA':\n","            y_data = 1\n","        else:\n","            y_data = 0\n","\n","        return x_data, y_data\n","\n","    def __len__(self):\n","        return int(len(self.image_name_list)/4)\n","\n","# #데이터 전처리\n","# transform = transforms.Compose(\n","#     [transforms.ToTensor(),transforms.Normalize((0.5 , 0.5, 0.5), (0.5, 0.5, 0.5))]\n","# )\n","\n","\n","#총 데이터 num :32 * 40 * 32 * 15 * 4(band), batch_size는 이의 약수, 논문에 명시됨\n","batch_size = 16\n","Data_Path_Train = \"./Data/Train_Data/\"\n","Data_Path_Validation = \"./Data/Validation_Data/\"\n","Data_Path_Test = \"./Data/Test_Data/\"\n","\n","\n","\n","#test_데이터, train_데이터 불러오고 저장\n","train_set = Arousal_Dataset(data_path = Data_Path_Train, train = True)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers = 0)\n","\n","test_set = Arousal_Dataset(data_path = Data_Path_Test, train = False)\n","\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n","\n","#output clasees\n","classes =  (0, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":446,"status":"ok","timestamp":1694949074035,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"AKK44tOYMqGx"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MSCB_CNN(nn.Module):\n","    def __init__(self):   \n","        super(SmallCNN, self).__init__()\n","\n","        ###for MSCB\n","        self.conv_L = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,5) , stride = 1, padding = 'same')\n","        self.conv_M = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,3), stride = 1, padding = 'same')\n","        self.conv_S = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        self.conv_raw = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        \n","        #Stride = 5 -> 그렇기에 kernel size 또한 5, 그러나 데이터의 규격이 5의 배수가 아니여서 4로 조정\n","        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4, padding = 'same')\n","\n","        # Convolutional layers\n","        self.conv = nn.Conv2d(in_channels = 56, out_channels = 112, kernel_size=(1,3), stride = 1, padding=3)\n","\n","        #flatten layer\n","        self.flat = nn.Flatten()\n","\n","        # Fully connected layers, 112 : final kernel num, /(4*4) striding number at pooling\n","        self.fc1 = nn.Linear(IMAGE_PIXEL_NUM * BAND_TYPE_NUM *112/(4*4)+ STAT_FEATURE_MAX_LEN, 400)\n","        self.fc2 = nn.Linear(400, 300)  \n","        # 2 output classes (0 and 1)«\n","        self.fc3 = nn.Linear(300, 2)\n","\n","        # Dropout layer to prevent overfitting\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","        \n","    def forward(self, x):\n","        #MSCB Block\n","        def MSCB(self, x):\n","            x_L = self.pool((torch.relu(self.conv_L(x))))\n","            x_M = self.pool((torch.relu(self.conv_M(x))))\n","            x_S = self.pool((torch.relu(self.conv_S(x))))\n","            x_raw = self.conv_raw((torch.relu(self.pool(x))))\n","\n","            ##2차원 이미지 합치기, kernerl수 증가\n","            x = torch.cat((x_L, x_M, x_S, x_raw), dim = 2)\n","\n","            return x\n","        \n","        def Conv_to_FCL(self,x):\n","            x = self.pool(torch.relu(self.conv(x)))\n","            x = self.flat(x)\n","\n","            return x\n","            \n","        # print(f\"initial x {x.shape}\")\n","        band_images, tensor_statistic_feature = x\n","\n","        theta_MSCB = MSCB(self, band_images[\"theta\"])\n","        alpha_MSCB = MSCB(self, band_images[\"alpha\"])\n","        beta_MSCB = MSCB(self, band_images[\"beta\"])\n","        gamma_MSCB = MSCB(self, band_images[\"gamma\"])\n","\n","        theta_features = Conv_to_FCL(theta_MSCB)\n","        alpha_features = Conv_to_FCL(alpha_MSCB)\n","        beta_features = Conv_to_FCL(beta_MSCB)\n","        gamma_features = Conv_to_FCL(gamma_MSCB)\n","        \n","        all_feature_map = torch.cat((theta_features,alpha_features,beta_features,gamma_features,tensor_statistic_feature), dim = 1)\n","\n","        x = torch.relu(self.fc1(all_feature_map))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x\n","\n","#initialize with MPS GPU\n","net = MSCB_CNN().to(device)\n","\n","\n","#Set Optimizer and loss function\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay = 0.02)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6c1a9e6e948346e08db35c1cdaa98142","57db90486c05493696ccd5dd12e1fa76","83f6d083d59f4370adbf5ed05163fe80","f3fc3fa138614c63849cc9788bcdfb63","5ae44592655b441b8a25dae83e08f514","4106fcd31a1b4ab8b7ea3a73d4feef59","8d439083db004ee2ad1e390d5498967c","1a37b57b03c141afb23bd96c9c60f2c8","44f30e031c6a4eb4ba80c4c47c11fcb2","6402a280118a477eb43772732850421e","cbbe9289e69b4786820ddb7f88d860ce"]},"id":"wK6lrJiHMqGx","outputId":"7c020a1b-5177-48b6-efbf-e6ca688f720f"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","#학습\n","start_flag = 1\n","for epoch in tqdm(range(500), desc= f\"epoch\"):   # 데이터셋을 수차례 반복합니다.\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(tqdm(train_loader, desc = \"learning!!!\"), 0):\n","\n","        if start_flag == 1:\n","            print(\"start learning!!!\")\n","            start_flag = 0\n","\n","        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n","        inputs, labels = data\n","\n","        #검증\n","        # print(f\"minibatch number : {i}, file's shape : {inputs.shape}\")\n","\n","        #move to GPU to calculate\n","        inputs, labels= inputs.to(device), labels.to(device)\n","        \n","        # 변화도(Gradient) 매개변수를 0으로 만들고\n","        optimizer.zero_grad()\n","\n","        # 순전파 + 역전파 + 최적화를 한 후\n","        outputs = net(inputs)\n","        # print(outputs.shape)\n","        # print(labels)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 통계를 출력합니다.\n","        running_loss += loss.item()\n","        if i % 600 == 599:    # print every 600 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","            \n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m55gfISW24vm"},"outputs":[],"source":["# ##Save Model\n","# PATH = \"/content/drive/MyDrive/models/\"\n","# torch.save(net.state_dict(), PATH)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"MEFECQ1bMqGy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the 10000 test images:  56.13 %\n"]}],"source":["#Accuracy Check\n","correct = 0\n","total = 0\n","# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images. labels = images.to(device), labels.to(device)\n","        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n","        outputs = net(images).to(device)\n","        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct / total : .2f} %')"]},{"cell_type":"markdown","metadata":{},"source":["***결과 정리 노트***\n","None : 58.74\n","Laplace : 41.09\n","CAR : 56.13"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"1a37b57b03c141afb23bd96c9c60f2c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4106fcd31a1b4ab8b7ea3a73d4feef59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f30e031c6a4eb4ba80c4c47c11fcb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57db90486c05493696ccd5dd12e1fa76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4106fcd31a1b4ab8b7ea3a73d4feef59","placeholder":"​","style":"IPY_MODEL_8d439083db004ee2ad1e390d5498967c","value":" 13%"}},"5ae44592655b441b8a25dae83e08f514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6402a280118a477eb43772732850421e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1a9e6e948346e08db35c1cdaa98142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57db90486c05493696ccd5dd12e1fa76","IPY_MODEL_83f6d083d59f4370adbf5ed05163fe80","IPY_MODEL_f3fc3fa138614c63849cc9788bcdfb63"],"layout":"IPY_MODEL_5ae44592655b441b8a25dae83e08f514"}},"83f6d083d59f4370adbf5ed05163fe80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a37b57b03c141afb23bd96c9c60f2c8","max":1394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44f30e031c6a4eb4ba80c4c47c11fcb2","value":184}},"8d439083db004ee2ad1e390d5498967c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbbe9289e69b4786820ddb7f88d860ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3fc3fa138614c63849cc9788bcdfb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6402a280118a477eb43772732850421e","placeholder":"​","style":"IPY_MODEL_cbbe9289e69b4786820ddb7f88d860ce","value":" 184/1394 [1:08:51&lt;7:32:00, 22.41s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
