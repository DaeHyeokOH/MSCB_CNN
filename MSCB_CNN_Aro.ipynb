{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":163658,"status":"ok","timestamp":1694949035452,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"ojnGvoh-MqGw"},"outputs":[],"source":["from torchvision.transforms import transforms\n","from torch.utils.data import Dataset\n","import pickle\n","import os\n","import torch\n","import numpy as np\n","from PIL import Image\n","import natsort\n","#set device\n","device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n","#constants\n","VIDEO = 40\n","PARTICIPANT = 32\n","CHANNEL = 32\n","SESSION = 63\n","STAT_FEATURE_MAX_LEN = 58\n","IMAGE_PIXEL_NUM = 496*369\n","BAND_TYPE_NUM = 4\n","\n","\n","#Make Datasets\n","class Arousal_Dataset(Dataset):\n","    def __init__(self, image_dir_path, train = None):\n","        super().__init__\n","        self.image_dir_path = image_dir_path\n","        image_name_list = os.listdir(image_dir_path)\n","        self.image_name_list = natsort.natsorted(image_name_list)\n","\n","    def __getitem__(self, index):\n","        ##image data\n","        #get image of theta to gamma\n","        #dictionary \"band type\" ->  tensor image\n","        band_images = {}\n","        band_types = ['alpha', 'beta', 'gamma', 'theta']\n","        for i in range(4):\n","            image_path = os.path.join(self.image_dir_path, self.image_name_list[4*index + i])\n","            image = np.array(Image.open(image_path))\n","            #remove alpha channel from .png file\n","            image = image[:,:,:3]\n","            tensor_image = torch.from_numpy(image).permute(2,0,1).float()\n","            band_images[band_types[i]] = tensor_image\n","\n","        image_name_split_list = self.image_name_list[4*index].split('_')\n","\n","        #locate information of files\n","        par, vid, cha, ses = int(image_name_split_list[0]), int(\n","            image_name_split_list[1]), int(image_name_split_list[2]), int(image_name_split_list[3])\n","        \n","        # statistic data,[Par, Vid, Cha, Ses] (32,40,32,15)\n","        with open('./Data/Statistic_Features_np.pkl', 'rb') as f:\n","            statistic_features = pickle.load(f)\n","\n","        statistic_feature = statistic_features[par-1][vid-1][cha-1][ses-1]\n","        #elong the legnth to (,58)\n","        statistic_feature = np.pad(statistic_feature, (0, STAT_FEATURE_MAX_LEN - len(statistic_feature)), 'constant', constant_values= 0)\n","        tensor_statistic_feature = torch.from_numpy(statistic_feature).float()\n","        \n","        #band images : dictionary, tensor images\n","        #tensor statistic feature : tensor 1 dim array\n","        x_data = band_images, tensor_statistic_feature\n","\n","        ##truth ground data form file name\n","        Truth_label = image_name_split_list[5]\n","\n","        if Truth_label[0 :2] == 'HA':\n","            y_data = 1\n","        else:\n","            y_data = 0\n","\n","        return x_data, y_data\n","\n","    def __len__(self):\n","        return int(len(self.image_name_list)/4)\n","\n","# #데이터 전처리\n","# transform = transforms.Compose(\n","#     [transforms.ToTensor(),transforms.Normalize((0.5 , 0.5, 0.5), (0.5, 0.5, 0.5))]\n","# )\n","\n","\n","#총 데이터 num :32 * 40 * 32 * 15 * 4(band), batch_size는 이의 약수, 논문에 명시됨\n","batch_size = 16\n","Data_Path_Train = \"./Data/Train_Data/\"\n","Data_Path_Validation = \"./Data/Validation_Data/\"\n","Data_Path_Test = \"./Data/Test_Data/\"\n","\n","\n","\n","#test_데이터, train_데이터 불러오고 저장\n","train_set = Arousal_Dataset(image_dir_path = Data_Path_Train, train = True)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers = 0)\n","\n","test_set = Arousal_Dataset(image_dir_path = Data_Path_Test, train = False)\n","\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n","\n","validation_set = Arousal_Dataset(image_dir_path= Data_Path_Validation, train = False)\n","\n","validation_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n","#output clasees\n","classes =  (0, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":446,"status":"ok","timestamp":1694949074035,"user":{"displayName":"DaeHyeok OH","userId":"18061477472662081697"},"user_tz":-540},"id":"AKK44tOYMqGx"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MSCB_CNN(nn.Module):\n","    def __init__(self):   \n","        super(MSCB_CNN, self).__init__()\n","\n","        ###for MSCB\n","        self.conv_L = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,5) , stride = 1, padding = 'same')\n","        self.conv_M = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,3), stride = 1, padding = 'same')\n","        self.conv_S = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        self.conv_raw = nn.Conv2d(in_channels = 3, out_channels = 14, kernel_size = (1,1), stride = 1, padding = 'same')\n","        \n","        #Stride = 5 -> 그렇기에 kernel size 또한 5, 그러나 데이터의 규격이 5의 배수가 아니여서 4로 조정, same padding : (kernerl_size/2, kerenel_size/2-1)\n","        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4, padding = (2,1))\n","\n","        # Convolutional layers\n","        self.conv = nn.Conv2d(in_channels = 56, out_channels = 112, kernel_size=(1,3), stride = 1, padding='same')\n","\n","        #flatten layer\n","        self.flat = nn.Flatten()\n","\n","        #batch_normalization\n","        self.batch_norm_MSCB = nn.BatchNorm2d(14)\n","        self.batch_norm_CNN = nn.BatchNorm2d(112)\n","        self.batch_norm_stat = nn.BatchNorm1d(58)\n","\n","        # Fully connected layers, 112 : final kernel num, /(4*4) striding number at pooling\n","        self.fc1 = nn.Linear(333370, 400)\n","        self.fc2 = nn.Linear(400, 300)  \n","        # 2 output classes (0 and 1)«\n","        self.fc3 = nn.Linear(300, 2)\n","\n","        # Dropout layer to prevent overfitting\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","        \n","    def forward(self, x):\n","        #MSCB Block\n","        def MSCB(self, x):\n","            x_L = self.pool(self.batch_norm_MSCB(torch.relu(self.conv_L(x))))\n","            x_M = self.pool(self.batch_norm_MSCB(torch.relu(self.conv_M(x))))\n","            x_S = self.pool(self.batch_norm_MSCB(torch.relu(self.conv_S(x))))\n","            x_raw = self.batch_norm_MSCB(self.conv_raw((torch.relu(self.pool(x)))))\n","\n","            ##2차원 이미지 합치기, kernerl수 증가\n","            x = torch.cat((x_L, x_M, x_S, x_raw), dim = 1)\n","\n","            return x\n","        \n","        def Conv_to_FCL(self,x):\n","            x = self.pool(self.batch_norm_CNN(torch.relu(self.conv(x))))\n","            x = self.flat(x)\n","\n","            return x\n","            \n","        # print(f\"initial x {x.shape}\")\n","        band_images, tensor_statistic_feature = x\n","        band_images['theta'], band_images['alpha'], band_images['beta'], band_images['gamma'], tensor_statistic_feature = band_images['theta'].to(device), band_images['alpha'].to(device), band_images['beta'].to(device), band_images['gamma'].to(device), tensor_statistic_feature.to(device) \n","        #stat feature normalization\n","        tensor_statistic_feature = self.batch_norm_stat(tensor_statistic_feature)\n","        theta_MSCB = MSCB(self, band_images[\"theta\"])\n","        alpha_MSCB = MSCB(self, band_images[\"alpha\"])\n","        beta_MSCB = MSCB(self, band_images[\"beta\"])\n","        gamma_MSCB = MSCB(self, band_images[\"gamma\"])\n","        # print(f\"MSCB shape : {theta_MSCB.shape}\")\n","\n","        theta_features = Conv_to_FCL(self, theta_MSCB)\n","        alpha_features = Conv_to_FCL(self, alpha_MSCB)\n","        beta_features = Conv_to_FCL(self, beta_MSCB)\n","        gamma_features = Conv_to_FCL(self, gamma_MSCB)\n","        \n","        all_feature_map = torch.cat((theta_features,alpha_features,beta_features,gamma_features,tensor_statistic_feature), dim = 1)\n","\n","        x = torch.relu(self.fc1(all_feature_map))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x\n","\n","#initialize with MPS GPU\n","net = MSCB_CNN().to(device)\n","\n","\n","#Set Optimizer and loss function\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay = 0.02)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.5)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6c1a9e6e948346e08db35c1cdaa98142","57db90486c05493696ccd5dd12e1fa76","83f6d083d59f4370adbf5ed05163fe80","f3fc3fa138614c63849cc9788bcdfb63","5ae44592655b441b8a25dae83e08f514","4106fcd31a1b4ab8b7ea3a73d4feef59","8d439083db004ee2ad1e390d5498967c","1a37b57b03c141afb23bd96c9c60f2c8","44f30e031c6a4eb4ba80c4c47c11fcb2","6402a280118a477eb43772732850421e","cbbe9289e69b4786820ddb7f88d860ce"]},"id":"wK6lrJiHMqGx","outputId":"7c020a1b-5177-48b6-efbf-e6ca688f720f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fcafde769ba4eb38a36de236a445f59","version_major":2,"version_minor":0},"text/plain":["epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98f51fe5f96b462b9aba4d9ae7b9f6ad","version_major":2,"version_minor":0},"text/plain":["learning!!!:   0%|          | 0/15311 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["start learning!!!\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/odaehyeok/Desktop/MI OA to Emotion/MSCB_CNN_Aro.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# 순전파 + 역전파 + 최적화를 한 후\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(outputs.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n","File \u001b[0;32m~/anaconda3/envs/palette/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32m/Users/odaehyeok/Desktop/MI OA to Emotion/MSCB_CNN_Aro.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#stat feature normalization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m tensor_statistic_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_stat(tensor_statistic_feature)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m theta_MSCB \u001b[39m=\u001b[39m MSCB(\u001b[39mself\u001b[39;49m, band_images[\u001b[39m\"\u001b[39;49m\u001b[39mtheta\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m alpha_MSCB \u001b[39m=\u001b[39m MSCB(\u001b[39mself\u001b[39m, band_images[\u001b[39m\"\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m beta_MSCB \u001b[39m=\u001b[39m MSCB(\u001b[39mself\u001b[39m, band_images[\u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m])\n","\u001b[1;32m/Users/odaehyeok/Desktop/MI OA to Emotion/MSCB_CNN_Aro.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMSCB\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     x_L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_MSCB(torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_L(x))))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     x_M \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_MSCB(torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_M(x))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     x_S \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_MSCB(torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_S(x))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/odaehyeok/Desktop/MI%20OA%20to%20Emotion/MSCB_CNN_Aro.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x_raw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_MSCB(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_raw((torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)))))\n","File \u001b[0;32m~/anaconda3/envs/palette/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/palette/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/anaconda3/envs/palette/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm.notebook import tqdm\n","#학습\n","num_epochs = 1\n","start_flag = 1\n","#check anomalies\n","torch.autograd.set_detect_anomaly(True)\n","\n","for epoch in tqdm(range(num_epochs), desc= \"epoch\"):   # 데이터셋을 수차례 반복합니다.\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(tqdm(train_loader, desc = \"learning!!!\"), 0):\n","\n","        if start_flag == 1:\n","            print(\"start learning!!!\")\n","            start_flag = 0\n","\n","        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n","        inputs, labels = data\n","\n","        #검증\n","        # print(f\"minibatch number : {i}, file's shape : {inputs.shape}\")\n","\n","        #move to GPU to calculate, inputs는 tuple, dict로 포장되어있기에 모델 내에서 .to(device)\n","        labels = labels.to(device)\n","        \n","        # 변화도(Gradient) 매개변수를 0으로 만들고\n","        optimizer.zero_grad()\n","\n","        # 순전파 + 역전파 + 최적화를 한 후\n","        outputs = net(inputs)\n","        # print(outputs.shape)\n","        # print(labels)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 통계를 출력합니다.\n","        running_loss += loss.item()\n","        if i % 100 == 99:    # print every 600 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","            \n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m55gfISW24vm"},"outputs":[],"source":["##Save Model\n","PATH = \"./models/\"\n","torch.save(net.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEFECQ1bMqGy"},"outputs":[],"source":["#Accuracy Check\n","correct = 0\n","total = 0\n","# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n","with torch.no_grad():\n","    for data in validation_loader:\n","        images, labels = data\n","        labels =labels.to(device)\n","        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n","        outputs = net(images).to(device)\n","        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct / total : .2f} %')"]},{"cell_type":"markdown","metadata":{},"source":["***결과 정리 노트***\n","None : 58.74\n","Laplace : 41.09\n","CAR : 56.13"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"1a37b57b03c141afb23bd96c9c60f2c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4106fcd31a1b4ab8b7ea3a73d4feef59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f30e031c6a4eb4ba80c4c47c11fcb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57db90486c05493696ccd5dd12e1fa76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4106fcd31a1b4ab8b7ea3a73d4feef59","placeholder":"​","style":"IPY_MODEL_8d439083db004ee2ad1e390d5498967c","value":" 13%"}},"5ae44592655b441b8a25dae83e08f514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6402a280118a477eb43772732850421e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1a9e6e948346e08db35c1cdaa98142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57db90486c05493696ccd5dd12e1fa76","IPY_MODEL_83f6d083d59f4370adbf5ed05163fe80","IPY_MODEL_f3fc3fa138614c63849cc9788bcdfb63"],"layout":"IPY_MODEL_5ae44592655b441b8a25dae83e08f514"}},"83f6d083d59f4370adbf5ed05163fe80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a37b57b03c141afb23bd96c9c60f2c8","max":1394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44f30e031c6a4eb4ba80c4c47c11fcb2","value":184}},"8d439083db004ee2ad1e390d5498967c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbbe9289e69b4786820ddb7f88d860ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3fc3fa138614c63849cc9788bcdfb63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6402a280118a477eb43772732850421e","placeholder":"​","style":"IPY_MODEL_cbbe9289e69b4786820ddb7f88d860ce","value":" 184/1394 [1:08:51&lt;7:32:00, 22.41s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
